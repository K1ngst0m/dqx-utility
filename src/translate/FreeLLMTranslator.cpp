#include "FreeLLMTranslator.hpp"
#include "translate/ApiKeys.h" // Generated by CMake from ApiKeys.h.in

#include <nlohmann/json.hpp>
#include <plog/Log.h>

namespace translate
{

// Static member initialization
std::atomic<std::size_t> FreeLLMTranslator::key_rotation_index_{ 0 };

const char* FreeLLMTranslator::providerName() const
{
    return "FreeLLM";
}

std::string FreeLLMTranslator::validateConfig(const BackendConfig& cfg) const
{
    // Only need to validate model is set (keys/URL are hardcoded)
    if (cfg.model.empty())
        return "Missing model selection";
    return {};
}

bool FreeLLMTranslator::hasValidRuntimeConfig() const
{
    // Only check model is set (keys/URL always valid)
    return !cfg_.model.empty();
}

ILLMTranslator::ProviderLimits FreeLLMTranslator::providerLimits() const
{
    ProviderLimits limits;
    limits.max_input_bytes = helpers::LengthLimits::OPENAI_API_MAX;
    return limits;
}

std::string FreeLLMTranslator::getNextApiKey() const
{
    // Atomic fetch-and-increment for thread-safe round-robin
    std::size_t index = key_rotation_index_.fetch_add(1, std::memory_order_relaxed) % API_KEY_COUNT;
    std::string key(FREELLM_API_KEYS[index]);

    // Check if key is empty (source build without keys)
    if (key.empty())
    {
        PLOG_ERROR << "FreeLLM API keys not configured. "
                   << "If building from source, pass keys via CMake: "
                   << "-DFREELLM_API_KEY_1=\"your_key\" etc.";
    }

    return key;
}

void FreeLLMTranslator::buildHeaders(const Job& job, std::vector<Header>& headers) const
{
    (void)job;
    headers.push_back({ "Content-Type", "application/json" });
    headers.push_back({ "Authorization", std::string("Bearer ") + getNextApiKey() });
}

std::string FreeLLMTranslator::buildUrl(const Job& job) const
{
    (void)job;
    return BASE_URL; // Always use hardcoded URL
}

void FreeLLMTranslator::buildRequestBody(const Job& job, const Prompt& prompt, nlohmann::json& body) const
{
    (void)job;
    body["model"] = cfg_.model; // User-selected model from UI

    // Check if using DeepSeek model (contains "ep-w8sv4r")
    bool is_deepseek = cfg_.model.find("ep-w8sv4r") != std::string::npos;
    bool is_qwen = cfg_.model.find("ep-c193qt") != std::string::npos;

    nlohmann::json messages = nlohmann::json::array();
    for (const auto& message : prompt.messages)
    {
        std::string role = "user";
        switch (message.role)
        {
        case Role::System:
            role = "system";
            break;
        case Role::Assistant:
            role = "assistant";
            break;
        default:
            role = "user";
            break;
        }

        // For Qwen: Add "/no_think" prefix to user messages to disable thinking
        std::string content = message.content;
        if (is_qwen && message.role == Role::User)
        {
            content = "/no_think\n" + content;
        }

        messages.push_back({
            { "role",    role    },
            { "content", content }
        });
    }
    body["messages"] = std::move(messages);
    body["temperature"] = 0.3;

    // For DeepSeek: Add chat_template_kwargs to disable thinking
    if (is_deepseek)
    {
        body["chat_template_kwargs"] = nlohmann::json::object();
        body["chat_template_kwargs"]["thinking"] = false;
    }
}

ILLMTranslator::ParseResult FreeLLMTranslator::parseResponse(const Job& job, const HttpResponse& resp,
                                                             Completed& out) const
{
    (void)job;
    ParseResult result;
    try
    {
        auto json = nlohmann::json::parse(resp.text);
        if (!json.contains("choices") || json["choices"].empty())
        {
            result.error_message = "missing choices in response";
            return result;
        }

        const auto& choice = json["choices"].at(0);
        if (!choice.contains("message") || !choice["message"].contains("content"))
        {
            result.error_message = "missing message content";
            return result;
        }

        out.text = choice["message"]["content"].get<std::string>();
        result.ok = true;
        return result;
    }
    catch (const std::exception& ex)
    {
        result.error_message = std::string("parse error: ") + ex.what();
        return result;
    }
}

std::string FreeLLMTranslator::connectionSuccessMessage() const
{
    return "Success: FreeLLM connection test passed, model responded correctly";
}

std::string FreeLLMTranslator::testConnectionImpl()
{
    if (cfg_.model.empty())
        return "Config Error: No model selected";

    // Use parent class implementation which will call buildUrl/buildHeaders/etc
    return ILLMTranslator::testConnectionImpl();
}

} // namespace translate
